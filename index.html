<html>
    <head>
        <style>
            html {
                scroll-behavior: smooth;
            }
            img {
                border:black solid
            }
            .container {
                display: flex;
            }
            body {
                background: #F0FFFF;
            }
        </style>
    </head>
    <body>
        <h1>Soccer Commentator</h1>
        <h2>A CS566 Computer Vision Student Project Attempting Object Tracking and Localization with Multiple Techniques</h2>
        <h3>Varun Sivakumar, Keiji Toriumi, Damien Kim</h3>
        <h3>
            <a href="https://docs.google.com/presentation/d/1v_VZspCd_MHnDR3Ap5I38BIB00CqaqBuK7_SCdr3lAc/edit?usp=sharing">
                Presentation Link
            </a>
            <br>
            <a href="https://github.com/ktoriumi107/CV-Soccer-Commentator-CS566/tree/keiji">
                Source Code Link
            </a>
        </h3>
        <div id="toc">
            <h2>Table of Contents</h2>
            <ul>
                <li><a href="#motivation">Motivation</a></li>
                <li><a href="#resources">Resources</a></li>
                <li><a href="#approach">Approach & Implementation</a></li>
                <li><a href="#results">Results</a></li>
                <li><a href="#discussion">Discussion</a></li>
                <li><a href="#conclusion">Conclusion & Future Work</a></li>
            </ul>
        </div>
        <hr>
        <h4 id="motivation">Motivation</h4>
        <p>
            Soccer matches have a lot going on; it can be difficult for the audience to keep track of them. As a result, live 
            commentaries are often selective and not holistic, failing to capture the whole picture. This poses a real problem 
            for blind audiences, who cannot independently keep track of the game. 
            <br>
            <br>
            This problem has been addressed in the past (<a href="https://aws.amazon.com/blogs/media/revolutionizing-fan-engagementcer-bundesliga-generative-ai-powered-live-commentary/">Amazon Web Services blog post</a>). 
            For example, Amazon Web Services has worked to develop a generative AI-powered live commentator. Although this model 
            had high accuracy in what it produces, it does not add any information beyond what live human commentators are able
            to add. The output is also harder to interpret since the data is not well-structured in a way that can be analyzed easily
            algorithmically. 
            <br>
            <br>
            We propose a data-based method using traditional computer vision methods that provides enough data to allow for thorough and thoughtful commentary.
            The purpose of this is to improve upon the live and AI-based commentators by providing data that gives the whole picture of the game. This benefits
            viewers since they will have more control over what specifically they want to hear about.
        </p>
        <hr>
        <h4 id="resources">Resources we found for this project:</h4>
        <p>
            <li><a href="https://dl.acm.org/doi/10.1145/3341105.3374063">(Paper) Automatic baseball commentary generation using deep learning</a></li>
            <p>We noticed using a series of neural networks trained for different tasks rather than conventional computer vision techniques.</p>
            <li><a href="https://arxiv.org/html/2407.08200v1">(Paper) Deep Understanding of Soccer Match Videos</a></li>
            <p>We noticed the Single Shot MultiBox Detector method for identifying objects and DeepSORT for tracking their motion.</p>
            <li><a href="https://www.dentsu.co.jp/en/showcase/voicewatch.html">(Article) Voice Watch: How AI Live Comments Can Change Spectator Sports</a></li>
            <p>We noticed the goal of generalizability. Voice Watch from Dentsu is a powerful tool for the visually impaired at racecar competitions, but there are hopes to make this tool work for other unordinary events (childrenâ€™s sporting events).</p>
            <li><a href="https://github.com/mkoshkina/jersey-number-pipeline">(Github Repo) jersey-number-pipeline</a></li>
            <li><a href="https://openaccess.thecvf.com/content/CVPR2024W/CVsports/papers/Koshkina_A_General_Framework_for_Jersey_Number_Recognition_in_Sports_Video_CVPRW_2024_paper.pdf">(Paper) A General Framework for Jersey Number Recognition in Sports Video</a></li>
            <p>These resources aided our implementation of the OCR pipline.</p>
            <li><a href="https://ieeexplore.ieee.org/document/5298809"> (Paper) Soccer Ball Tracking Using Dynamic Kalman Filter with Velocity Control</a></li>
            <p>This resource aided our implementation of the ball tracking algorithm.</p>
        </p>
        <hr>
        <h4 id="approach">Approach and Implementation</h4>
        <p>
            We had the core tasks of:
            <li>Object detection for players and the ball</li>
            <li>OCR (optical character recognition) for recognizing jersey numbers and team identity</li> 
            <li>Object tracking </li>
            <li>Localization to identify locations on the field </li>
            <li>Have output: concise, human-readable commentary lines in the console</li>

            <br><br>
            This is summarized in a diagram of the functionality of the pipeline.

            <figure>
                <img src="visuals/diagram.png">
                <figcaption>
                    The diagram of an example use of the pipeline used for the soccer commentator.
                </figcaption>
            </figure>
 
            <br><br>
            
            <div class="container">
                <div style="margin:auto">
                    For object detection, we ended up utilizing Ultralytics YOLO AI framework model for its ease of use and good performance.
                    <br>
                    Keiji's first approach was to use a neural network to generate bounding boxes for the players and the ball. 
                    YOLO11 was available on the Roboflow platform and provided a dataset of 663 images for training.
                </div>
                <div>
                    <figure>
                        <img src="visuals/neural_network_result.png" style="width:600px;height:600px">
                        <figcaption>An image of its success in distinguishing players on the field and the sidelines:</figcaption>
                    </figure>
                </div>
            </div>
            <br><br><br>
            <figure>
                <img src="visuals/graph.png">
                <figcaption>
                    The training graph above demonstrates a good performance of 99% accuracy on players but only 57% accuracy on the ball. 
                    Keiji presumed the ball's fast movement caused the lower accuracy.
                </figcaption>
            </figure>
 

            <br><br>
            For OCR, we initally tested PyTesseract, EasyOCR, and the jersey-number-pipeline Github repo for inspiration. 
            See <a href="#problems">The problems we encountered section</a> for why we skipped PyTesseract.
            <br><br>
            <div id="OCR">
                Varun's first approach was learning the OCR algorithm and pipeline from the Github repo. Despite the prolific image dataset (SoccerNet Jersey Number Recognition) and 
                ReID algorithm to find outliers of unidentifiable jersey numbers to be excluded from OCR, the time to extract features 
                from our images took far too long: 12 images per hour. 
            </div>
            <br><br>
            <div class="container">
                <div style="margin:auto">
                    Varun thus implemented the core OCR pipeline steps manually: start with a cropped image of a player's back, detect the player, detect the ball, and perform OCR using the EasyOCR library. 
                    Note, YOLOv8 was used for the detection tasks.
                    The rationale was that performing OCR on a smaller region would compute quicker. However, the images weren't getting cropped correctly.
                </div>
                <figure>
                    <img src="visuals/cropped_error.jpg" style="width:400px;height:400px">
                    <figcaption>An example of the cropped image</figcaption>
                </figure>
            </div>
            <br><br>
            <div class="container">
                <img src="visuals/success_ocr.png" style="width:400px;height:400px">
                <div style="margin:auto">
                    Varun resorted to keep the image as the bounding box around the player with the result to the left:
                </div>
            </div>
            <br><br><br>
            
            <div class="container">
                <div style="margin:auto">
                    The next step was to expand this to all players in the image, but it was unsuccessful:
                </div>
                <figure>
                    <img src="visuals/entire_image_ocr_result.jpg" style="width:1200px;height:500px">
                    <figcaption>ReID could be implemented here to filter out jersey positions with unidentifiable numbers.
                        Another alternate solution is to continuously run OCR on every player every frame, waiting for the jersey number to become identifiable. 
                        This cost is the speed of the system computation.</figcaption>
                </figure>
            </div>

            <br><br>
            For object tracking, our methods were different for the players and the ball.
            <br>
            For the players, we simply continued to use what YOLOv8 provided, and it was successful.
            <br>
            <div id="ball_tracking">
                For the ball, YOLOv8 wasn't effective. The model failed to detect the ball when it was moving and in posession of a player.
                To address this, Varun referred to an online paper describing the use of Kalman filtering to estimate a soccer ball's velocity and motion trajectory,
                and implemented a similar approach to predict the ball's position even when YOLO failed to detect it.
                This was more effective, but because of an issue where the tracking was off due to the ball's sudden change in direction when kicked,
                improvements were made to snap the ball tracker to the nearest player's leg.
                This was more effective, but still failed in the case when the ball is kicked into the goal. It goes past the defender and the algorithm snaps it to the defender's leg.
            </div>
            <br><br>

            <div class="container">
                <div style="margin:auto">For localization, Keiji first approached it using the Hough Line Transform:</div>
                <img src="visuals/hough.png" style="width:900px;height:600px">
            </div>
            <div class="container">
                <div style="margin:auto">His second approach was RANSAC:</div>
                <img src="visuals/ransac.png" style="width:900px;height:600px">
            </div>
            
            <br><br>
            <div class="container">
                <img src="visuals/console_output.png" style="width:400px;height:400px">
                <div style="margin:auto">For output, we used print statements for the players identity and their following action:</div>
            </div>
        </p>
        <hr>
        <h4 id="results">Results</h4>
        <p>
            We reconfigured the program on Google Colab to use its free GPU!
            <br>
            <div class="container">
                <figure>
                    <figcaption><b>Object Tracking with an attempt at OCR</b></figcaption>
                    <video width="320" height="240" controls>
                        <source src="visuals/1080soccer.mov" type="video/mp4">
                        Could not load video.
                    </video>
                </figure>
                <figure>
                    <figcaption><b>Object Tracking and OCR with Localization</b></figcaption>
                    <video width="320" height="240" controls>
                        <source src="visuals/1080soccer_with_overlays.mov" type="video/mp4">
                        Could not load video.
                    </video>
                </figure>
            </div>
        </p>
        <hr>
        <h4 id="discussion">Discussion</h4>
        <h5>OCR</h5>
        <div class="container">
            <figure>
                <img src="visuals/failed_1.jpeg" style="width:400px;height:400px;">
                <figcaption>PyTesseract was a powerful tool, but it was not successful with the noisy and distorted images of player jerseys.</figcaption>
            </figure>

            <figure>
                <figcaption>
                    We tried to use traditional methods of contours and edge detection to begin the process of identifying players. 
                    But, we struggled getting the program to differentiate what was needed (players, balls, jerseys, and jersey numbers)
                    and what wasn't needed (field, people and text in the background, noise on the field).
                </figcaption>
                <img src="visuals/failed_2.png" style="width:400px;height:400px;">
                <figcaption>
                    Even filtering strategies, like checking the area of the contour (min max constraint) were not effective, 
                    considering the players and ball can change size depending on the camera.
                </figcaption>
            </figure>
        </div>

        <h5>OCR continued</h5>
        <p>
            An OCR model we wanted to work with took too long to extract features. We wanted to use OCR on cropped images sparingly, 
            but the cropping was incorrect.
            Our main problem was the result of OCR: only one player was correctly identified.
            <a href="#OCR">Click here to read more.</a>
        </p>
        <h5>Ball Tracking</h5>
        <p>YOLO11 had 57% accuracy on the ball detection. <a href="#ball_tracking">Click here to read about our ball tracking problem.</a></p>
        <hr>
        <h4 id="conclusion">Future prospects and Conclusions</h4>
        <h5>Summary of contributions</h5>
        <ul>
            <li>OCR that integrates YOLOv8 player detection with EasyOCR for jersey recognition.</li>
            <li>Custom ball-tracking module using a Kalman filter with velocity prediction and player-leg snapping logic.</li>
            <li>Localization using both Hough Transform and RANSAC to detect field boundaries and lines.</li>
            <li>Real-time commentary generator translating the data from the pipeline into readable sentences.</li>
        </ul>

        <h5>Limitations</h5>
                <ul>
                    <li>The OCR failed to recognize the jersey number in most cases. This is likely due to the low resolution in
                    the small bounding boxes that players were confined to in combination with the noisy and distorted view of the jerseys.</li>
                    <li>The localization was unable to map the players to known field locations like the penalty box and center circle. It instead used positions relative to other
                    objects and the known corner of the field. This reduced accuracy because a homography was not able to be computed since there were not enough points and the keypoints
                    used for coordinate estimation were further away from each player. This meant that there was minimal validation of the player
                    positions by comparing them to other known field locations. This was rectified by basing the motion tracking on the movement
                    of objects relative to each other and the field boundaries, rather than known marks on the field.</li>
                    <li>The accuracy of this pipeline was only able to be tested by-hand. There was no available dataset that included labels for
                    the objects as well as their ground-truth field locations from a camera broadcast taken at an oblique angle that is similar to
                    what would be seen in real games. Based on the raw volume of data produced, validating this pipeline was time-consuming and error prone.
                    We chose to instead focus on key elements for each section of the pipeline. </li>
                </ul>
        <h5>Future Work</h5>
        <p>
            <li>The ball tracking algorithm fails to track the ball when it is kicked into the goal. Adding this 
                functionality would allow for direct goal tracking. This problem would involve a vision system 
                capable of detecting the ball within the net. This is difficult because the ball and net are the 
                same color and the ball is easily covered.</li>
            <li>Generalizability was an end goal that we had in our proposal. We have not yet tested on similar sports, 
                but our reliance on the YOLOv8 model and its dataset for soccer for our object detection helps us 
                understand that the more generalizable it is, the more expensive it would be to compute. Having a 
                neural network specific to a sport makes sense in this way. It's more accurate and cheaper to work with.</li>
            <li>Different neural network models. For this project, using traditional computer vision methods sounded better, 
                but using machine learning turned out to be much easier to implement. YOLO was just one AI framework model we 
                found, there are more that may allow for robust data generation while still being accurate and easy to implement.</li>
        </p>
        <h5>Video overview of project</h5>
        <figure>
            <video controls width="600">
                <source src="visuals/presentation_video.mp4" type="video/mp4">
                Could not load video.
            </video>
        </figure>
        <hr>
    </body>
</html>
